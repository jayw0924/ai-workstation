#!/usr/bin/env python3
"""
Autonomous Research Agent
Searches web, analyzes sources, generates comprehensive reports
"""
import os
import sys
from datetime import datetime
from pathlib import Path
from anthropic import Anthropic
from dotenv import load_dotenv
from ddgs import DDGS
import trafilatura
import time

# Load environment
load_dotenv('../.env')

class ResearchAgent:
    def __init__(self, output_dir="research_outputs"):
        self.client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def search_web(self, query, max_results=10):
        """Search the web using DuckDuckGo"""
        print(f"\nüîç Searching web for: '{query}'")
        print("-" * 60)
        
        results = []
        with DDGS() as ddgs:
            search_results = list(ddgs.text(query, max_results=max_results))
            
            for idx, result in enumerate(search_results, 1):
                results.append({
                    'title': result.get('title', 'No title'),
                    'url': result.get('href', ''),
                    'snippet': result.get('body', 'No description')
                })
                print(f"{idx}. {result.get('title', 'No title')}")
                print(f"   {result.get('href', '')}")
                print()
        
        return results
    
    def scrape_content(self, url):
        """Scrape full content from URL"""
        try:
            downloaded = trafilatura.fetch_url(url)
            text = trafilatura.extract(downloaded, include_comments=False)
            return text if text else ""
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error scraping {url}: {e}")
            return ""
    
    def analyze_sources(self, query, sources):
        """Have Claude analyze all sources and synthesize findings"""
        print(f"\nü§ñ Analyzing {len(sources)} sources with Claude...")
        print("-" * 60)
        
        # Build context from sources
        context_parts = []
        for idx, source in enumerate(sources, 1):
            context_parts.append(f"""
Source {idx}: {source['title']}
URL: {source['url']}
Content: {source['content'][:3000]}...  
""")
        
        context = "\n\n---\n\n".join(context_parts)
        
        prompt = f"""You are a research analyst. I've gathered information from multiple web sources about this topic:

TOPIC: {query}

SOURCES:
{context}

Please provide a comprehensive research report that:
1. Synthesizes the key findings across all sources
2. Identifies main themes and patterns
3. Notes any contradictions or disagreements between sources
4. Provides specific citations (e.g., "According to Source 1...")
5. Highlights the most important insights
6. Suggests areas for further research

Format your response as a well-structured report with sections and clear citations."""

        message = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4096,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return message.content[0].text
    
    def generate_report(self, query, search_results, analysis):
        """Generate markdown report"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""# Research Report: {query}

**Generated:** {timestamp}  
**Agent:** Autonomous Research Agent  
**Sources Analyzed:** {len(search_results)}

---

## Executive Summary

{analysis}

---

## Sources

"""
        
        for idx, source in enumerate(search_results, 1):
            report += f"""
### Source {idx}: {source['title']}

**URL:** {source['url']}  
**Snippet:** {source['snippet']}

"""
        
        report += f"""

---

## Methodology

1. Searched web using DuckDuckGo
2. Retrieved {len(search_results)} relevant sources
3. Scraped full content from each source
4. Analyzed using Claude Sonnet 4
5. Synthesized findings with citations

---

*Report generated by Autonomous Research Agent*
"""
        
        return report
    
    def research(self, query, max_sources=5, save_to_kb=True):
        """Execute full research workflow"""
        print("\n" + "=" * 60)
        print(f"üî¨ AUTONOMOUS RESEARCH AGENT")
        print("=" * 60)
        print(f"\nTopic: {query}")
        print(f"Max sources: {max_sources}")
        print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Step 1: Search web
        search_results = self.search_web(query, max_results=max_sources)
        
        if not search_results:
            print("\n‚ùå No search results found!")
            return None
        
        # Step 2: Scrape content from each source
        print(f"\nüìÑ Scraping content from {len(search_results)} sources...")
        for idx, result in enumerate(search_results, 1):
            print(f"  {idx}/{len(search_results)} Scraping: {result['url']}")
            content = self.scrape_content(result['url'])
            result['content'] = content
            time.sleep(1)  # Be polite to servers
        
        # Filter out sources with no content
        valid_sources = [s for s in search_results if s['content']]
        print(f"\n‚úÖ Successfully scraped {len(valid_sources)}/{len(search_results)} sources")
        
        if not valid_sources:
            print("\n‚ùå No content could be scraped from sources!")
            return None
        
        # Step 3: Analyze with Claude
        analysis = self.analyze_sources(query, valid_sources)
        
        # Step 4: Generate report
        print(f"\nüìù Generating report...")
        report = self.generate_report(query, valid_sources, analysis)
        
        # Step 5: Save report
        safe_filename = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in query)
        safe_filename = safe_filename[:50]  # Limit length
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{safe_filename}.md"
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\n‚úÖ Report saved: {filepath}")
        
        # Step 6: Optionally save to knowledge base
        if save_to_kb:
            kb_path = Path("../markdown-rag/data/markdown/research_reports")
            kb_path.mkdir(exist_ok=True)
            kb_file = kb_path / filename
            with open(kb_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"üìö Also saved to knowledge base: {kb_file}")
            print("   (Run ingest_enhanced.py to add to RAG system)")
        
        print("\n" + "=" * 60)
        print("‚ú® Research complete!")
        print("=" * 60)
        
        return {
            'query': query,
            'report': report,
            'filepath': filepath,
            'sources': valid_sources
        }

def main():
    if len(sys.argv) < 2:
        print("Usage: python research_agent.py 'your research topic'")
        print("\nExample:")
        print("  python research_agent.py 'What are the latest developments in RAG systems?'")
        sys.exit(1)
    
    query = " ".join(sys.argv[1:])
    
    agent = ResearchAgent()
    result = agent.research(query, max_sources=5, save_to_kb=True)
    
    if result:
        print(f"\nüìñ Read full report at: {result['filepath']}")

if __name__ == "__main__":
    main()
